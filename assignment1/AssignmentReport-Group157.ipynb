{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an outline for your report to ease the amount of work required to create your report. Jupyter notebook supports markdown, and I recommend you to check out this [cheat sheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet). If you are not familiar with markdown.\n",
    "\n",
    "Before delivery, **remember to convert this file to PDF**. You can do it in two ways:\n",
    "1. Print the webpage (ctrl+P or cmd+P)\n",
    "2. Export with latex. This is somewhat more difficult, but you'll get somehwat of a \"prettier\" PDF. Go to File -> Download as -> PDF via LaTeX. You might have to install nbconvert and pandoc through conda; `conda install nbconvert pandoc`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%latex\n",
    "## task 1a)\n",
    "\n",
    "$$\n",
    "C(w) = - \\frac{1}{N} \\sum_{n = 1}^{N} y^n \\ln(\\hat{y}^n) + (1-y^n) \\ln(1-\\hat{y}^n)\n",
    ",  \\hat{y} = f(x) = \\frac{1}{1+e^{-w^Tx}}\n",
    "$$\n",
    "\n",
    "Hint gives us that \n",
    "\n",
    "$$ \n",
    "\\frac{\\partial f(x^n)}{\\partial w_{i}} = x_i^n f(x^n)(1-f(x^n))\n",
    "$$\n",
    "\n",
    "Partial derivate\n",
    "$$ \n",
    "\\frac{\\partial}{\\partial w_j} \\ln(\\hat{y}^n)= \\frac{\\partial}{\\partial w_j}  \\ln(f(x^n)) = \\frac{1}{f(x^n)} \\frac{\\partial}{\\partial w_j}f(x^n) = \\frac{1}{f(x^n)} x_j^n f(x^n)(1-f(x^n)) = x_{j}^{n} (1-f(x^n))\n",
    "$$\n",
    "\n",
    "$$ \n",
    "\\frac{\\partial}{\\partial w_j} \\ln(1-\\hat{y}^n) = \\frac{\\partial}{\\partial w_j} \\ln(1-f_w(x^n))= \\frac{1}{1-f_w(x^n)} \\frac{\\partial }{\\partial w_j} (1-f_w(x^n))= \\frac{-1}{1-f(x^n)} x_j^n f(x^n)(1-f(x^n)) =- x_{j}^{n} f(x^{n}) \n",
    "$$\n",
    "\n",
    "This gives\n",
    "$$ \n",
    "\\frac{\\partial}{\\partial w_j} C^n(w)=  -\\frac{\\partial}{\\partial w_j}(y^n \\ln(\\hat{y}^n) + (1-y^n) \\ln(1-\\hat{y}^n)) =  -y^n x_{j}^{n} (1-f(x^n)) + (1-y^n) x_{j}^{n} f(x^{n})  = -(y^n - \\hat{y}^n) x_j^n\n",
    "$$\n",
    "\n",
    "$$ \n",
    "\\frac{\\partial}{\\partial w_j} C^n(w)= -(y^n - \\hat{y}^n) x_j^n\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1b)\n",
    "$$\n",
    "\\frac{\\partial}{\\partial w_{kj}} C^n(w) = -\\frac{1}{K}x_j^n (y_k^n-\\hat{y}_k^n).\n",
    "$$\n",
    "\n",
    "$$\n",
    "C(w) =  \\frac{1}{N} \\sum_{n=1}^N C^n(w)=  -\\frac{1}{N\\cdot K} \\sum_{n=1}^N \\sum_{k=1}^K y_k^n \\ln(\\hat{y_k^n})\n",
    "$$\n",
    "\n",
    "We observe\n",
    "$$\n",
    "\\hat{y}_k^n = \\frac{e^{z_k^n}}{\\sum_{k'=1}^K e^{z_{k'}^n}}\n",
    "$$\n",
    "where\n",
    "$$\n",
    "z_k^n = w_k^T x^n\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial w_{kj}} C^n(w) = \\frac{\\partial C^n(w)}{\\partial \\hat{y}_k^n} \\frac{\\partial \\hat{y}_k^n}{\\partial z_k^n} \\frac{\\partial z_k^n}{\\partial w_{kj}}\n",
    "$$ \n",
    "Hints give us that\n",
    "$$\n",
    "\\sum_{k=1}^K y_k^n =1\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\ln(\\frac{a}{b}) = \\ln(a)-\\ln(b)\n",
    "$$\n",
    "\n",
    "Two different cases: \n",
    "\n",
    "case i) i = k \n",
    "\n",
    "\n",
    "$$\n",
    "\\partial \\frac{\\hat{y}_i^n}{z_k^n} = \\frac{e^{z_k^n}}{\\sum_{k'=1}^K e^{z_{k'}^n}}  -\\frac{e^{z_k^n} e^{z_k^n}}{(\\sum_{k'=1}^K e^{z_{k'}^n})^2} = \\hat{y}_k^n(1-\\hat{y}_k^n)\n",
    "$$\n",
    "\n",
    "\n",
    "and case ii) i != k\n",
    "$$\n",
    "\\partial \\frac{\\hat{y}_i^n}{z_k^n} = \\frac{-e^{z_i^n} e^{z_k^n}}{(\\sum_{i} e^{z_k^n})^2} = - \\hat{y}_i^n *\\hat{y}_k^n\n",
    "$$\n",
    "\n",
    "Finally we get the following:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial w_{kj}} C^n(w)  =-\\frac{1}{N}\\sum_{n=1}^N y_k^n (x_i^n - \\frac{e^{a_k^n}x_i^n}{\\sum_{k'}{e^{a_k^n}}} - y_k^n\\frac{e^{a_k^n}x_i^n}{\\sum_{k'}{e^{a_k^n}}})\n",
    "$$\n",
    "$=-\\frac{1}{N}\\sum_{n=1}^N x_i^n(y_k^n-\\hat{y}_k^n)$\n",
    "$$\n",
    "=-x_i^n(y_k^n-\\hat{y}_k^n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2b) \n",
    "\\n\\n\n",
    "![](task2b_binary_train_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2c)\n",
    "![](task2b_binary_train_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2d)\n",
    "\n",
    "The early stopping kicked in after 16 epochs when running with shuffle=True, and when shuffle=False it stops at epoch 33. What we see in the diagram is with shuffling set to true\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2e)\n",
    "In the plot we see that the spikes are almost nonexistent when using shuffling. This is because when shuffling is set to true the model receives a completely new random batch at each epoch. When there isn't shuffling the model spikes whenever it receives a batch that it seemingly does not predict particularly well. We say that shuffling helps keep the model general. Shuffling data means that we reduce the amount of times we go into a local minima during gradient descent since the points are chosen more at random.\n",
    "![](task2e_train_accuracy_shuffle_difference.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3b)\n",
    "![](task3b_softmax_train_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3c)\n",
    "![](task3b_softmax_train_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3d)\n",
    "We see tendencies of overfitting, since the accuracy grows higher on the training data than on the validation data. This means that the model becomes better at predicting on the training data but does not improve at predicting validation on the test data. In our case however, the validation accuracy still has a growing trend for 500 epochs so while it is showing a general trend of growing towards overfitting, I would argue it is not overfitting. Some deviation between  validation data and training data accuracy will always be a reality. I would argue this model has a good amount of generalization after training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4a)\n",
    "\n",
    "The new cost function is as given in (9) \n",
    "$$\n",
    "J(w) = C(w) + \\lambda R(w) $ where $R(w)\n",
    "$$\n",
    "is the complexity penalty and $\\lambda$ is the strength of the regularization.\n",
    "\n",
    "L2  ridge regression is implemented as\n",
    "$$\n",
    "R(w)=\\|w\\|^2= \\sum_{i,j} w_{i,j}^2 = \\sum_{i=1}^{I} \\sum_{j=1}^{J} w_{i,j}^2. \n",
    "$$\n",
    "Its gradient can be calculated as\n",
    "$$\n",
    "\\frac{\\partial}{\\partial w}R(w) = \\sum_{i=1}^{I} \\sum_{j=1}^{J} w_{i,j}^2 \\frac{\\partial}{\\partial w}w_{i,j} = \\sum_{i=1}^{I} 2 w_i = 2\\sum_{i=1}^{I} w_i\n",
    "$$\n",
    "\n",
    "Which gives our cost function derivative the following form\n",
    "$$\n",
    "\\frac{\\partial}{\\partial w}J(w) = -x_i^n(y_k^n-\\hat{y}_k^n) + 2 \\lambda \\sum_{i=1}^{I} w_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4b)\n",
    "In the plot below we see the weights with lambda=1.0 and lambda=0.0, after a L2 term is added to the cost function.\n",
    "Without L2 penalization the weights have more noise as we see below. However, when we set a lambda=1.0 we see a reduced noise due to the L2 function penalizing the weight function thus reducing the noise in the model. To reiterate, each input is given less weight and the cost function is greedier in its allocation. This is what results in the denoising.\n",
    "![](task4b_softmax_weight.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4c)\n",
    "Below we find the plot with different lambda values. note that early stopping is turned on which is why some stop earlier. Low lambda values give better results as we see. However, lambda=0 is worse than small lambda values meaning the L2 regularization is helping the model.\n",
    "\n",
    "![](task4c_l2_reg_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4d)\n",
    "\n",
    "By adding the L2 function to the cost we punish the weight sizing, and this in turn results in lower accuracy. It may seem counterproductive to use an L2 function for this reason, but in turn it makes the model much more generalized and reduces overfitting. Lambda needs to be chosen with care as too high or too low can result in under- or overfitting.\n",
    "\n",
    "![](task4d_l2_reg_norms.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4e)\n",
    "\n",
    "Increasing lambda decreases the length of the L2 norm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python36964biteef8d8053aa54c489f0d9a087cd5ec38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}